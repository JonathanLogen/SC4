## GPTech Support
- A small, scrappy startup is offering an automated IT helpdesk service with a bot powered by GPT-3. They send your
question, along with some secret information, as a prompt to GPT. What's the secret sauce? Find out using... an
injection attack? With natural language!?

- Hint:
    - An injection attack against GPT prompts is called... "prompt injection"!
    - The flag is in the first sentence of the prompt.

## Setup 
- The file ``secrets_config.json`` must contain an an API key for https://openai.com/. 
- Create a free account and use the API key from this. 
